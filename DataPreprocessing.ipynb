{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate, signal\n",
    "import warnings\n",
    "import mne\n",
    "import random\n",
    "import torch\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw data preprocessing\n",
    "We segmented the original tsv file to obtain its valid information, and intercepted the data of the corresponding time period in which it viewed the videos for segmentation to obtain the relevant eye movement data about the 10 videos viewed in disordered order for each subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_time_2_stamp(time):\n",
    "    time = time.split(':')\n",
    "    assert len(time) == 3\n",
    "    stamp    = 0\n",
    "    time_h   = 60*60*1000*int(time[0])\n",
    "    time_m   = 60*1000*int(time[1])\n",
    "    time_s   = time[2].split('.')[0]\n",
    "    time_s   = 1000*int(time_s)\n",
    "    time_ms  = time[2].split('.')[1][:3]\n",
    "    time_ms  = int(time_ms)\n",
    "    stamp    = time_h + time_m + time_s + time_ms\n",
    "    return stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_trigger(eye_data, trigger_data):\n",
    "\n",
    "    start_time = eye_data['LocalTimeStamp'].values[0] \n",
    "    base       = from_time_2_stamp(start_time)\n",
    "\n",
    "    trigger = []\n",
    "    for i in range(trigger_data.shape[0]):\n",
    "        cur_trigger = []\n",
    "        cur_trigger.append(int(trigger_data.iloc[i, 0]))\n",
    "        stamp = from_time_2_stamp(trigger_data.iloc[i, 1].split(' ')[1])\n",
    "        # cur_trigger.append((stamp - base)*1000)\n",
    "        cur_trigger.append((stamp - base))\n",
    "        trigger.append(cur_trigger)\n",
    "    return trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_stamp(trigger, start, end):\n",
    "    start_idx = []\n",
    "    end_idx   = []\n",
    "    for i in range(len(trigger)):\n",
    "        if trigger[i][0] == start:\n",
    "            start_idx.append(trigger[i][1])\n",
    "        if trigger[i][0] == end:\n",
    "            end_idx.append(trigger[i][1])\n",
    "    return start_idx, end_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index_for_a_trigger(time_col, trigger):\n",
    "    left = 0\n",
    "    right = len(time_col) - 1\n",
    "    while right - left > 1:\n",
    "        mid = int(left + (right - left) / 2)\n",
    "        if time_col[mid] == trigger:\n",
    "            return mid\n",
    "        elif time_col[mid] < trigger:\n",
    "            left = mid\n",
    "        else:\n",
    "            right = mid\n",
    "    if trigger <= (time_col[left] + time_col[right]) / 2:\n",
    "        return left\n",
    "    else:\n",
    "        return right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EYE_preprocess(COLUMN_USED, path = './raw_data', clip_path = './raw_data_clip'):\n",
    "    \"\"\"\n",
    "    Main function of eye movement file preprocessing\n",
    "    \"\"\"\n",
    "    if os.path.exists(clip_path):\n",
    "        shutil.rmtree(clip_path)\n",
    "    os.mkdir(clip_path)\n",
    "    id_list = os.listdir(path)\n",
    "    id_list.sort()\n",
    "    for id in id_list:\n",
    "        print(\"Begin to process person {}.\".format(id))\n",
    "        id_path = os.path.join(path, id)\n",
    "        clip_path_one=os.path.join(clip_path,id)\n",
    "        os.mkdir(clip_path_one)\n",
    "        eye_file, trigger_file, info_file = None, None, None\n",
    "        for file in os.listdir(id_path):\n",
    "            if file.split('.')[1] == 'tsv':\n",
    "                eye_file = os.path.join(id_path, file)\n",
    "            if file.split('.')[1] == 'csv' and file.split('_')[-2] == 'trigger':\n",
    "                trigger_file = os.path.join(id_path, file)\n",
    "            if file.split('.')[1] == 'csv' and file.split('_')[-2] == 'save':\n",
    "                info_file = os.path.join(id_path, file)\n",
    "        eye_data = pd.read_csv(eye_file, sep='\\t', low_memory = False)\n",
    "        eye_data = eye_data[COLUMN_USED]\n",
    "        trigger_data = pd.read_csv(trigger_file)\n",
    "        trigger = read_trigger(eye_data, trigger_data)\n",
    "        stamp = find_stamp(trigger, 1, 2)\n",
    "        for i in range(len(stamp[0])):\n",
    "            start_ind = find_index_for_a_trigger(eye_data['RecordingTimestamp'], stamp[0][i])\n",
    "            end_ind = find_index_for_a_trigger(eye_data['RecordingTimestamp'], stamp[1][i])\n",
    "            # clip_data = eye_data[stamp[0][i]:stamp[1][i]]\n",
    "            clip_data = eye_data[start_ind:end_ind]\n",
    "            save_path = os.path.join(clip_path_one, id[2] + '_' + id + '_' + str(i) + '.csv')\n",
    "            clip_data=clip_data[['GazePointLeftX (ADCSpx)','GazePointRightX (ADCSpx)', 'GazeEventType']]\n",
    "            clip_data.to_csv(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Processing\n",
    "The next step is to determine whether the eye movement signal is focused or not based on the degree of focusing, i.e., the positional difference between the left and right eyes. We retain the *GazeEventType* attribute of the eye movement data and compute the positional difference between the left and right eyes, and use a combination of the standard deviation and the mean to make a judgment.\n",
    "\n",
    "In addition, considering the large noise level and the existence of segments that did not successfully record valid eye movement data, we firstly interpolate them and finally smooth them to modify the intervals that are too short (usually caused by noise) to obtain a smoother focused and unfocused interval. \n",
    "\n",
    "We end up using the *Focus* attribute to record whether it's focused or not. To make subsequent EEG cutting and alignment easier, we chose to splice the 10-video slices to obtain a total data file about each subject. \n",
    "\n",
    "In addition, since the images are played in disorganized order, they need to be sorted chronologically according to the timestamp at the end of the merge to ensure that the eye-tracking labels are saved strictly in chronological order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Denoise(data, interval = 10):\n",
    "    sequence = data['Focus'].tolist()\n",
    "    i = 0\n",
    "    while i < len(sequence):\n",
    "        if sequence[i] == 0:\n",
    "            count = 0\n",
    "            j = i\n",
    "            while j < len(sequence) and sequence[j] == 0:\n",
    "                count += 1\n",
    "                j += 1\n",
    "            if count <= interval:\n",
    "                for k in range(i, j):\n",
    "                    sequence[k] = 1\n",
    "            i = j\n",
    "        else:\n",
    "            i += 1\n",
    "    data['Focus'] = sequence\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Focus(data, interval = 10):\n",
    "    for i in range(len(data['GazePointLeftX (ADCSpx)'])):\n",
    "        if (pd.isnull(data['GazePointLeftX (ADCSpx)'][i]) and not pd.isnull(data['GazePointRightX (ADCSpx)'][i])) or (not pd.isnull(data['GazePointLeftX (ADCSpx)'][i]) and pd.isnull(data['GazePointRightX (ADCSpx)'][i])):\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                data['GazePointLeftX (ADCSpx)'][i] = np.nan\n",
    "                data['GazePointRightX (ADCSpx)'][i] = np.nan\n",
    "    data['GazePointLeftX (ADCSpx)'] = data['GazePointLeftX (ADCSpx)'].interpolate(method = 'cubic')\n",
    "    data['GazePointRightX (ADCSpx)'] = data['GazePointRightX (ADCSpx)'].interpolate(method = 'cubic')\n",
    "    data['Gaze difference'] = data['GazePointLeftX (ADCSpx)']-data['GazePointRightX (ADCSpx)']\n",
    "    mean = data['Gaze difference'].mean()\n",
    "    std = data['Gaze difference'].std()\n",
    "    data['Focus'] = 1\n",
    "    for i in data.index.tolist():\n",
    "        data['Focus'][i] = 1 if data['Gaze difference'][i] <= (mean + std) else 0\n",
    "    data = data.drop(columns = ['GazePointLeftX (ADCSpx)', 'GazePointRightX (ADCSpx)'])\n",
    "    return Denoise(data = data, interval = interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Display_focus(data, name):\n",
    "    fig, ax1 = plt.subplots(figsize = (5,4))\n",
    "    ax1.set_xlabel('Timestamp')\n",
    "    ax1.set_ylabel('Gaze difference', color='b')\n",
    "    ax1.plot(data.index.tolist(),data['Gaze difference'], color = 'b', ls = '-', lw = 1)\n",
    "    # ax1.set_xticks(data.index.tolist())\n",
    "    ax1.tick_params(axis = 'y', labelcolor = 'b')\n",
    "    ax2 = ax1.twinx()  \n",
    "    ax2.set_ylabel('Focus', color='r') \n",
    "    ax2.plot(data.index.tolist(),data['Focus'], color = 'r', ls = ':', lw = 0.5)\n",
    "    ax2.tick_params(axis = 'y', labelcolor = 'r')\n",
    "    fig.tight_layout() \n",
    "    plt.title(name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Label_process(pathbase, savebase, display = False, interval = 10):\n",
    "    \"\"\"\n",
    "    Main function of label processing\n",
    "    \"\"\"\n",
    "    if os.path.exists(savebase):\n",
    "        shutil.rmtree(savebase)\n",
    "    os.mkdir(savebase)\n",
    "    person = os.listdir(pathbase)\n",
    "    person.sort()\n",
    "    for j in person:\n",
    "        print(\"Begin to process person {}.\".format(j))\n",
    "        perpath = os.path.join(pathbase,j)\n",
    "        eye_list = os.listdir(perpath)\n",
    "        eye_list.sort()\n",
    "        persave = os.path.join(savebase,j)\n",
    "        os.mkdir(persave)\n",
    "        for i in eye_list:\n",
    "            path = os.path.join(perpath,i)\n",
    "            data = pd.read_csv(path)\n",
    "            if data['GazePointLeftX (ADCSpx)'].isnull().all() and data['GazePointRightX (ADCSpx)'].isnull().all():\n",
    "                print(\"{} has no valid data!\".format(i))\n",
    "                continue\n",
    "            data = Focus(data = data, interval = interval)\n",
    "            if display:\n",
    "                Display_focus(data = data, name = i)\n",
    "            data = data.drop(columns = 'Gaze difference')\n",
    "            savepath = os.path.join(persave,i)\n",
    "            data.columns = ['Timestamp', 'GazeEventType', 'Focus']\n",
    "            for k in data.index.tolist():\n",
    "                data['GazeEventType'][k] = 1 if data['GazeEventType'][k] == 'Fixation' else 0\n",
    "            data.to_csv(savepath, index = False)\n",
    "            print(\"Finished processing {}!\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment of EEG raw data and eye movement label\n",
    "In the following, we preprocessed the EEG signals. Considering that the sampling frequencies of EEG and eye movement are different, we first need to downsample the EEG signal so that it can match the eye movement label perfectly. According to the event label, the data of the corresponding time period of viewing the video is taken out and downsampled, at this time, the data is strictly in chronological order, and then, it is merged with the eye movement labels to become a complete preliminary dataset.\n",
    "\n",
    "It should be noted that, since each sample contains two label attributes, in order to be more accurate, we set that only when the attributes of *GazeEventType* and *Focus* are both 1, we will determine that it is a positive sample, otherwise, it is a negative sample, and we use new labels to indicate the positive and negative attributes. After such an operation it is still possible that the interval is too short, i.e., there is noise, so we perform smoothing again.\n",
    "\n",
    "This is followed by the separation of positive and negative sample intervals. We separated the positive and negative samples into two separate data sets based on the labels. However, considering that if the intervals are too short it may cause problems in subsequent feature extraction, which is usually due to the fact that too short intervals lead to the inability to obtain enough frequency bands when they are subjected to the short-time Fourier transform, we screened them again, and obtained more robust data after eliminating the shorter intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Down_sample(perpath, eye_path):\n",
    "    for name in os.listdir(perpath):\n",
    "        if name.split('.')[1] == 'cnt':\n",
    "            if name[-9:-4] == 'curry':\n",
    "                cntpath_curry = os.path.join(perpath, name)\n",
    "            else:\n",
    "                cntpath = os.path.join(perpath, name)\n",
    "    cntindex = mne.io.read_raw_cnt(cntpath)\n",
    "    cntdata = mne.io.read_raw_cnt(cntpath_curry)\n",
    "    raw_data, _ = cntdata.get_data(return_times = True)\n",
    "    events, dict = mne.events_from_annotations(cntindex)\n",
    "    start, end = events[events[:,2] == dict['1']][:,0], events[events[:,2] == dict['2']][:,0]\n",
    "    videos = []\n",
    "    eye_list = os.listdir(eye_path)\n",
    "    for i in range(start.shape[0]):\n",
    "        video = raw_data[:,start[i]:end[i]].tolist()\n",
    "        eye_file = pd.read_csv(os.path.join(eye_path, eye_list[i]))\n",
    "        for k in range(len(video)):\n",
    "            video[k] = signal.resample(video[k], eye_file.shape[0]).tolist()\n",
    "        video = np.array(video)\n",
    "        videos.append(video.T)\n",
    "    return videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Aligning(videos, labels):\n",
    "    datasets = videos\n",
    "    datasets1 = datasets[:,0:32]\n",
    "    datasets2 = datasets[:,33:42]\n",
    "    datasets3 = datasets[:,43:(datasets.shape[1]-3)]\n",
    "    datasets = np.concatenate((datasets1, datasets2, datasets3), axis = 1)\n",
    "    final_labels = []\n",
    "    for i in range(labels.shape[0]):\n",
    "        if labels[i][0] == 1 and labels[i][1] == 1:\n",
    "            final_labels.append(1)\n",
    "        else:\n",
    "            final_labels.append(0)\n",
    "    i = 0\n",
    "    interval = 10\n",
    "    sequence = final_labels\n",
    "    while i < len(sequence):\n",
    "        if sequence[i] == 0:\n",
    "            count = 0\n",
    "            j = i\n",
    "            while j < len(sequence) and sequence[j] == 0:\n",
    "                count += 1\n",
    "                j += 1\n",
    "            if count <= interval:\n",
    "                for k in range(i, j):\n",
    "                    sequence[k] = 1\n",
    "            i = j\n",
    "        else:\n",
    "            i += 1\n",
    "    final_labels = np.array(sequence)\n",
    "    final_labels = np.expand_dims(final_labels, axis = 1)\n",
    "    datasets = np.concatenate((datasets, final_labels), axis = 1)\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Segmentation(dataset):\n",
    "    pos_data, neg_data = [], []\n",
    "    start = end = 0\n",
    "    for i in range(dataset.shape[0]-1):\n",
    "        if dataset[i][-1] != dataset[i+1][-1] or (i+1) % 5001 == 0:\n",
    "            end = i\n",
    "            if end - start + 1 >= 50: \n",
    "                if dataset[i][-1] == 1:\n",
    "                    pos_data.append(dataset[start:end+1,:])\n",
    "                else:\n",
    "                    neg_data.append(dataset[start:end+1,:])\n",
    "                start = i+1\n",
    "    return pos_data, neg_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction\n",
    "In the following, we need to find the relevant features of the EEG signals, so we perform a full-length short-time Fourier transform for all intervals, which is extracted as 5 samples of feature 67 channels, reshape them into a column, and subsequently add labels and merge the set of positive and negative samples to get the final usable dataset about each subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_psd(energy_graph, freq_bands, sample_freq, stft_n=256):\n",
    "    start_index = int(np.floor(freq_bands[0] / sample_freq * stft_n))\n",
    "    end_index = int(np.floor(freq_bands[1] / sample_freq * stft_n))\n",
    "    ave_psd = np.mean(energy_graph[:, start_index - 1:end_index] ** 2, axis=1)\n",
    "    return ave_psd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_psd_feature(raw_data, freq_bands, stft_n=256):\n",
    "    n_channels, n_samples = raw_data.shape\n",
    "    psd_feature = np.zeros((len(freq_bands), n_channels))\n",
    "    start_index, end_index = 0, n_samples\n",
    "    window_data = raw_data[:, start_index:end_index]\n",
    "    hdata = window_data * signal.hann(n_samples)\n",
    "    fft_data = np.fft.fft(hdata, n=stft_n)\n",
    "    energy_graph = np.abs(fft_data[:, 0: int(stft_n / 2)])\n",
    "    for band_index, band in enumerate(freq_bands):\n",
    "        band_ave_psd = get_average_psd(energy_graph, band, n_samples, stft_n)\n",
    "        psd_feature[band_index, :] = band_ave_psd\n",
    "    return psd_feature.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extracton(data, freq_band, label, stft_n=256):\n",
    "    for i in range(len(data)):\n",
    "        data[i] = extract_psd_feature(data[i].T[0:data[i].shape[1]-1,:], freq_bands = freq_band, stft_n = stft_n)\n",
    "        data[i] = data[i].reshape(1, data[i].size).squeeze().tolist()\n",
    "        if label == 1:\n",
    "            data[i].append(1)\n",
    "        else:\n",
    "            data[i].append(0)\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Feature_extracton(path, savebase, data_path, freq_bands):\n",
    "    \"\"\"\n",
    "    Main function of alignment and feature extraction\n",
    "    \"\"\"\n",
    "    if os.path.exists(data_path):\n",
    "        shutil.rmtree(data_path)\n",
    "    os.mkdir(data_path)\n",
    "    person = []\n",
    "    for name in os.listdir(path):\n",
    "        person.append(name)\n",
    "    person.sort()\n",
    "    for j in person:\n",
    "        print(\"Begin to process person {}.\".format(j))\n",
    "        perpath = os.path.join(path,j)\n",
    "        eye_path = os.path.join(savebase,j)\n",
    "        os.mkdir(os.path.join(data_path, j))\n",
    "        videos = Down_sample(perpath = perpath, eye_path = eye_path)\n",
    "        id_list = os.listdir(eye_path)\n",
    "        indices = []\n",
    "        for i in range(len(id_list)):\n",
    "            index = []\n",
    "            tmp = pd.read_csv(os.path.join(eye_path, id_list[i]))\n",
    "            index.append(tmp['Timestamp'][0])\n",
    "            index.append(os.path.join(eye_path, id_list[i]))\n",
    "            indices.append(index)\n",
    "        indices = sorted(indices, key=(lambda x:x[0]))\n",
    "        for i in range(len(indices)):\n",
    "            label = pd.read_csv(indices[i][1])[['GazeEventType', 'Focus']]\n",
    "            label = np.array(label)\n",
    "            datasets = Aligning(videos = videos[i], labels = label)\n",
    "            pos_data, neg_data = Segmentation(dataset = datasets)\n",
    "            pos_data = Extracton(data = pos_data, freq_band = freq_bands, label = 1)\n",
    "            neg_data = Extracton(data = neg_data, freq_band = freq_bands, label = 0)\n",
    "            data = np.concatenate((pos_data, neg_data), axis = 0)\n",
    "            data = data[~np.isnan(data).any(axis = 1)]\n",
    "            np.random.shuffle(data)\n",
    "            print(data.shape)\n",
    "            np.save(os.path.join(data_path, j , str(i) + '.npy'), data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project configurations\n",
    "Here we define the various necessary paths, whether or not data preprocessing is required for a particular session, and the parameters that need to be read corresponding to certain data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "COLUMN_USED = ['RecordingTimestamp', 'LocalTimeStamp',\n",
    "               'GazePointLeftX (ADCSpx)', 'GazePointRightX (ADCSpx)', 'GazeEventType']\n",
    "preprocess, label, feature_extracton = False, False, False\n",
    "path = './raw_data'\n",
    "clip_path = './raw_data_clip'\n",
    "savebase = './raw_data_focus'\n",
    "data_path = './dataset'\n",
    "freq_bands = [(1,4),(4,8),(8,14),(14,31),(31,49)]\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Operations\n",
    "Here we select the module corresponding to the execution of data preprocessing by the configuration above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin to process person huangsiye_20210529_1.\n",
      "Used Annotations descriptions: ['1', '2', '3', '4', '5', '6']\n",
      "(49, 311)\n",
      "(57, 311)\n",
      "(61, 311)\n",
      "(39, 311)\n",
      "(110, 311)\n",
      "(101, 311)\n",
      "(19, 311)\n",
      "(43, 311)\n",
      "(73, 311)\n",
      "(59, 311)\n",
      "Begin to process person huangsiye_20210531_2.\n",
      "Used Annotations descriptions: ['1', '2', '3']\n",
      "(72, 311)\n",
      "(67, 311)\n",
      "(70, 311)\n",
      "(137, 311)\n",
      "(62, 311)\n",
      "(30, 311)\n",
      "(48, 311)\n",
      "(40, 311)\n",
      "(106, 311)\n",
      "(26, 311)\n",
      "Begin to process person huangsiye_20210604_3.\n",
      "Used Annotations descriptions: ['1', '2', '3']\n",
      "(31, 311)\n",
      "(48, 311)\n",
      "(84, 311)\n",
      "(34, 311)\n",
      "(13, 311)\n",
      "(80, 311)\n",
      "(38, 311)\n",
      "(52, 311)\n",
      "(101, 311)\n",
      "(52, 311)\n",
      "Begin to process person liangjie_20210424_2.\n",
      "Used Annotations descriptions: ['1', '2', '3', '6']\n",
      "(139, 311)\n",
      "(153, 311)\n",
      "(128, 311)\n",
      "(238, 311)\n",
      "(98, 311)\n",
      "(54, 311)\n",
      "(68, 311)\n",
      "(67, 311)\n",
      "(150, 311)\n",
      "(88, 311)\n",
      "Begin to process person liuzhiwei_20210608_1.\n",
      "Used Annotations descriptions: ['1', '2', '3', '4', '5', '6']\n",
      "(86, 311)\n",
      "(88, 311)\n",
      "(115, 311)\n",
      "(42, 311)\n",
      "(158, 311)\n",
      "(163, 311)\n",
      "(34, 311)\n",
      "(66, 311)\n",
      "(171, 311)\n",
      "(25, 311)\n",
      "Begin to process person liuzhiwei_20210611_2.\n",
      "Used Annotations descriptions: ['1', '2', '3']\n",
      "(102, 311)\n",
      "(84, 311)\n",
      "(88, 311)\n",
      "(96, 311)\n",
      "(76, 311)\n",
      "(27, 311)\n",
      "(36, 311)\n",
      "(50, 311)\n",
      "(130, 311)\n",
      "(19, 311)\n",
      "Begin to process person tana_20210425_1.\n",
      "Used Annotations descriptions: ['1', '2', '3', '4', '5', '6']\n",
      "(46, 311)\n",
      "(35, 311)\n",
      "(75, 311)\n",
      "(22, 311)\n",
      "(117, 311)\n",
      "(49, 311)\n",
      "(26, 311)\n",
      "(40, 311)\n",
      "(64, 311)\n",
      "(54, 311)\n",
      "Begin to process person zengcheng_20210615_1.\n",
      "Used Annotations descriptions: ['1', '2', '3', '4', '5', '6']\n",
      "(82, 311)\n",
      "(133, 311)\n",
      "(201, 311)\n",
      "(49, 311)\n",
      "(183, 311)\n",
      "(178, 311)\n",
      "(56, 311)\n",
      "(64, 311)\n",
      "(171, 311)\n",
      "(46, 311)\n",
      "Begin to process person zengjingyao_20210604_2.\n",
      "Used Annotations descriptions: ['1', '2', '3']\n",
      "(104, 311)\n",
      "(167, 311)\n",
      "(115, 311)\n",
      "(208, 311)\n",
      "(84, 311)\n",
      "(51, 311)\n",
      "(71, 311)\n",
      "(71, 311)\n",
      "(158, 311)\n",
      "(99, 311)\n",
      "Begin to process person zhengwenxin_20210415_1.\n",
      "Used Annotations descriptions: ['1', '2', '3', '4', '5', '6']\n",
      "(62, 311)\n",
      "(79, 311)\n",
      "(107, 311)\n",
      "(27, 311)\n",
      "(126, 311)\n",
      "(68, 311)\n",
      "(28, 311)\n",
      "(37, 311)\n",
      "(71, 311)\n",
      "(6, 311)\n",
      "Begin to process person zhengwenxin_20210429_2.\n",
      "Used Annotations descriptions: ['1', '2', '3']\n",
      "(77, 311)\n",
      "(110, 311)\n",
      "(137, 311)\n",
      "(83, 311)\n",
      "(91, 311)\n",
      "(37, 311)\n",
      "(60, 311)\n",
      "(45, 311)\n",
      "(104, 311)\n",
      "(32, 311)\n",
      "Begin to process person zhengwenxin_20210505_3.\n",
      "Used Annotations descriptions: ['1', '2', '3']\n",
      "(90, 311)\n",
      "(94, 311)\n",
      "(150, 311)\n",
      "(49, 311)\n",
      "(19, 311)\n",
      "(125, 311)\n",
      "(47, 311)\n",
      "(75, 311)\n",
      "(93, 311)\n",
      "(43, 311)\n",
      "Begin to process person zhoucaizhi_20210419_3.\n",
      "Used Annotations descriptions: ['1', '2', '3']\n",
      "(71, 311)\n",
      "(103, 311)\n",
      "(136, 311)\n",
      "(39, 311)\n",
      "(34, 311)\n",
      "(82, 311)\n",
      "(27, 311)\n",
      "(89, 311)\n",
      "(84, 311)\n",
      "(18, 311)\n"
     ]
    }
   ],
   "source": [
    "if preprocess:\n",
    "    EYE_preprocess(COLUMN_USED=COLUMN_USED,path = path, clip_path = clip_path)\n",
    "if label:\n",
    "    Label_process(pathbase = clip_path, savebase = savebase, display = False, interval = 10)\n",
    "if feature_extracton:\n",
    "    Feature_extracton(path = path, savebase = savebase, data_path = data_path, freq_bands = freq_bands)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
